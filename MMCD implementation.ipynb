{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import data, color\n",
    "from skimage.transform import downscale_local_mean\n",
    "from skimage.measure import block_reduce\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation ,Flatten, Conv2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I made a file that only contained labels taken from trainLabels.csv given with the dataset on kaggle site.\n",
    "#     this OnlyLabels.txt had a label on each line\n",
    "\n",
    "# Because i had  limited resources (only cpu with 8 gb ram, i could not take the image and process it. \n",
    "#     so instead of taking the whole image i set the width and height to 256*256. tho this way i lost a part of image \n",
    "#      but still i was able to achieve good results.)\n",
    "\n",
    "# PreProcessing\n",
    "with open('OnlyLabels.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "tempfile=[]\n",
    "for line in contents:\n",
    "    tempfile.append(line.split('\\t'))\n",
    "tempfile=np.array(tempfile)\n",
    "\n",
    "counter=1\n",
    "data=[]\n",
    "for t in tempfile:\n",
    "    FileToOpen=t[0]\n",
    "    FileToOpen+='.bytes'\n",
    "    with open(FileToOpen) as file:\n",
    "        out = file.readlines()\n",
    "        count = 0\n",
    "        arr = ''\n",
    "        for line in out:\n",
    "            line2 = line.split(' ', 1)[1]\n",
    "            line3 = line2.split(' ')\n",
    "            for l in line3:\n",
    "                try:\n",
    "                    binval = '{:08b}'.format(int(l, 16))\n",
    "                    arr += str(binval)\n",
    "                except:\n",
    "                    count += 1\n",
    "           \n",
    "        arr2 = [arr[start:start+8] for start in range(0, len(arr), 8)]\n",
    "\n",
    "        # making decimal values and saving them in file\n",
    "        decval_list = []\n",
    "        for a in arr2:\n",
    "            decval = int(a, 2)\n",
    "            decval_list.append(decval)\n",
    "            #file2.write(str(decval)+\" \")\n",
    "\n",
    "        lengthoflist = len(decval_list)\n",
    "\n",
    "        width = 256\n",
    "        #height = int(lengthoflist/16)\n",
    "        height = 256  \n",
    "        Matrix = np.zeros(shape=(height, width))\n",
    "\n",
    "        wcount = 0\n",
    "        hcount = 0\n",
    "        count = 0\n",
    "        for li in decval_list:\n",
    "            try:\n",
    "                Matrix[hcount][wcount] = li\n",
    "            except:\n",
    "                print(\"File used : \" ,FileToOpen,\". Number of values used \", count , \", out of \" , lengthoflist)\n",
    "                break\n",
    "            wcount += 1\n",
    "            count += 1\n",
    "            if wcount == width:\n",
    "                hcount += 1\n",
    "                wcount = 0\n",
    "        decMat = np.array(Matrix)\n",
    "        data.append(decMat)\n",
    "    print(counter,\" files have been processed\")\n",
    "    counter += 1 \n",
    "    print()\n",
    "data=np.array(data)\n",
    "print(data.shape)\n",
    "print(\"Preprocessing is  done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.array(data)\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels.txt have observation number and reponse class in each line\n",
    "import os\n",
    "filenames2 = []\n",
    "with os.scandir() as i:\n",
    "    for entry in i:\n",
    "        if entry.is_file():\n",
    "            #just excluding the name of the files that i had in my current directory and were not part of data.\n",
    "            if entry.name != 'OnlyLables.txt' and entry.name != 'lables.txt' and entry.name != 'lables2.py' and entry.name != 'untitled.ipynb':\n",
    "                filenames2.append(entry.name)\n",
    "                # print((entry.name))\n",
    "\n",
    "# print(filenames)\n",
    "\n",
    "with open('lables.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "a = []\n",
    "count = 0\n",
    "count2 = 0\n",
    "for line in contents:\n",
    "    temp = line.split('\\t')\n",
    "    for str1 in filenames2:\n",
    "        if temp[0] in str1:\n",
    "            a.append(temp)\n",
    "            count += 1\n",
    "            if(temp[0] == ''):\n",
    "                count2 += 1\n",
    "index = 0\n",
    "labelarr=[]\n",
    "for i in a:\n",
    "    i[1] = i[1].replace('\\n', \"\")\n",
    "    labelarr+=i[1]\n",
    "print(len(labelarr))\n",
    "labels=np.array(labelarr)\n",
    "print(labels.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = data.reshape((1000, 256, 256,1))   \n",
    "print(X_train.shape)\n",
    "# enc = OneHotEncoder()\n",
    "# enc.fit(y_train)  \n",
    "# y_train  = enc.transform(labels)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=X_train\n",
    "print(labels.shape)\n",
    "labelscopy=labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_int = [int(x) for x in labels]\n",
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data).reshape(-1)\n",
    "    return np.eye(nb_classes)[targets]\n",
    "\n",
    "y_train = indices_to_one_hot(np.array(labels_int)-1, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Applying VGG\n",
    "\n",
    "vgg = Sequential()\n",
    "vgg.add(Conv2D(3, (3, 3), padding='same',input_shape=X_train.shape[1:]))\n",
    "vgg.add(Activation('relu'))\n",
    "\n",
    "_vgg = VGG16(weights='imagenet', include_top=False)\n",
    "#or\n",
    "# _vgg = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "counter=0\n",
    "for layer in _vgg.layers:\n",
    "    layer.trainable = False\n",
    "    counter+=1\n",
    "print(\"VGG's \", counter , \" layers are not added to the layer\")\n",
    "vgg.add(_vgg)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.add(Flatten())\n",
    "\n",
    "vgg.add(Dense(4096,activation='relu'))\n",
    "vgg.add(Dense(9,activation=\"softmax\"))\n",
    "\n",
    "vgg.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs  = 15\n",
    "model_info = vgg.fit(X_train, y_train,  epochs=epochs, batch_size=batch_size)\n",
    "vgg.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vari=vgg.predict(X_test.reshape((X_test.shape[0],32,32,1)))\n",
    "vari2=np.argmax(vari,axis=1)\n",
    "print(\"starting evaluating\")\n",
    "vgg.evaluate(X_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
